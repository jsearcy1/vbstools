Dear editor and referees, 

Thank you very much for the comments on our paper - they helped
improve its clarity.
We will follow the editor's and one referee's recommendation and
resubmit our updated manuscript to Phys. Rev. D.  Please find our
detailed answers to your comments and questions below, starting with
"===>".

Best regards,
The authors.


Re: LK14763 
Determination of the WW polarization fractions in pp rightarrow W  pm 
W  pm jj using a deep machine learning technique 
by Jacob Searcy, Lillian Huang, Marc-Andr\'e Pleier, et al. 
Dear Dr. Pleier,

The above manuscript has been reviewed by our referees.  A critique
drawn from the reports appears below.  On this basis, we judge that
while the work probably warrants publication in some form, it does not
meet the Physical Review Letters criteria of impact, innovation, and
interest.

The paper, with revision as appropriate, might be suitable for
publication in Physical Review.  If you submit the paper to Physical
Review, the editors of that journal will make the decision on
publication of the paper, and may seek further review; however, our
complete file will be available.

If you submit this manuscript or a revision of it to Physical Review,
be sure to respond to all referee comments and cite the code number
assigned to the paper to facilitate transfer of the records.
If you feel that you can overcome or refute the criticism, you may
resubmit to Physical Review Letters.  Please accompany any resubmittal
by a summary of the changes made and a brief response to all
recommendations and criticisms.

Yours sincerely,
Kevin Dusling
Associate Editor
Physical Review Letters
Email: prl@aps.org
http://journals.aps.org/prl/
IMPORTANT: Editorial "Review Changes"

http://journals.aps.org/prl/edannounce/PhysRevLett.111.180001


----------------------------------------------------------------------

Report of Referee A -- LK14763/Searcy

----------------------------------------------------------------------

This is an interesting, very readable paper. The use of deep learning
NNs to actually calculate an under constrained quantity in particle
physics is new to me.  I have a number of serious comments though and
it works best to attach them to the plots:

Fig 1a: it compares LL, -- and ++ according to the labeling, but in
fact in only shows a single cos theta* distribution for +, - and L.

===> This plot shows the cos theta* distribution for the first lepton
in LL, -- and ++ events. The distribution for the first and the second
lepton are the same since we do not label two leptons in a particular
order.  We prefer to make comparisons in 1D instead of 2D for clarity
and modified the caption to clarify what is being shown by adding:
"(a) and (c) are the projections of the 2D distributions onto one of
the two leptons, which are identical to the projections onto the other
lepton due to arbitrary sorting."

Fig 1b: the log scale is necessary but makes a meaningful comparison
with the NN impossible.

===> We agree and modified the caption to emphasize the scale
difference for these two plots by adding: "$R_{pT}$ templates for the
corresponding polarization states *with log scaling* (b)". A more
quantitative comparison is provided in the paragraph before the
conclusion, where we give the uncertainty on the fitted LL fraction
using both variables.


Fig 1c: the x-axis is cos theta NN and not cos theta*. Since this is
the distribution used in the template fit, it is not possible to cut
on it to suppress background, so why show it. The actual background
shape can be derived from data using fully reconstructed (3 lepton)
background and removing the appropriate lepton. Probably this figure
can go.

===> We corrected the label of the x-axis to cos theta NN. While it
may be possible to use 3-lepton data events to derive the cos theta NN
distribution as you suggested, we feel this figure is relevant because
it shows the discrimination power of this variable between the signal
and dominant background.

Fig2: Confusing, if cos theta_1 NN and cos theta_2 NN are correlated
maybe 3 different 2D plots would be best, if not a 1-D distribution
would be a lot clearer.

===> We followed your suggestion to add three 2D plots to show the
differences in cos theta NN 1 vs. 2 correlations, but we still prefer
to keep the original 1D plot that clearly demonstrates the fitting
procedure as well as the fraction from each template.

Fig3: part a and b are not realistic and therefore not relevant and
can go, only use c. The x-axis should be integrated luminosity.

===> While we agree that parts a) and b) are not realistic, we think
these two scenarios provide additional information about corresponding
detector effects on the LL fraction measurement and hence prefer to
keep these two plots. For example, when comparing a) to b) one can see
that the cuts used strongly impact the significance implying future
optimization potential. Similarly, the transition from b) to c) shows
the degradation of the final sensitivity due to finite detector
resolution and detection efficiency. This has been clarified in the
text, adding the statement "A comparison of scenarios (a) and (b) to
(c) illustrates potential gains in sensitivity through cut
optimization and improved detector performance."  We updated the
x-axis labels to "Integrated Luminosity".

In general, even the realism of c) is limited. To obtain a data set of
several ab-1 years of running with high pile-up is needed. The issue
of pile-up, leading to misidentification of the tagging jets and
decreasing the precision of the missing Et measurement is not
addressed at all. While I expect the deep learning NN still to
outperform the Rt method, the most realistic and difficult scenario
does not look good for a meaningful LL measurement

===> Concerning the realism of c) due to pileup-effects, mis-tagged
jets are not a major concern in this case, since they are generally
easily rejected by increased pt cuts. This can be performed with
almost no loss of efficiency because of the already relatively high
M(j,j) cuts. We rely on delphes for the MissingET resolution, which
could miss some effects of high pile-up. We feel that the mitigation
of pile-up is an interesting topic which requires accurate simulation
and detector specific reconstruction which we do not (and cannot)
investigate in this paper. We pointed out this limitation by adding
the statement: "This detector smearing adds some degree of realism,
but could neglect various effects due to the large number of
overlapping interactions during the high luminosity LHC runs. Since
these effects are often mitigated with specific reconstruction
techniques and need detailed detector modeling, we leave studies of
this nature to dedicated efforts by the experiments."

----------------------------------------------------------------------

Report of Referee B -- LK14763/Searcy

----------------------------------------------------------------------

The paper outlines the use of a machine learning technique,
specifically, the use of deep neural networks, to measure WW
polarization fractions in events with two same sign W bosons and two
hadronic jets. These measurements and study of vector boson scattering
are important to carry out at the LHC and particularly at the high
luminosity LHC.

Neural networks method has been used extensively in High Energy
Physics for almost 25 years now. Recently, the CMS collaboration used
it in many aspects of Higgs boson searches and discovery. It is not
surprising that deep neural networks with several hidden layers do
somewhat better than the single hidden layer or shallow networks
because of a large number of parameters in the deep networks and
building up of the mapping from input to output over several layers.
Therefore, I am not convinced of the novelty or urgency for the paper
to be published in physical review letters. The paper describes an
interesting and useful study and an interesting application. It might
be useful to include more details regarding the deep neural network
training, provide comparisons with performance of single hidden layer
network to clearly demonstrate the gains, and publish a detailed paper
in Physical Review D.

===> Thank you for finding our application interesting - we will
respond in detail to your questions below.  Neural networks most
commonly are used combining many observables to form a new
discriminant to separate signal and backgrounds, while in this paper
we exploit the idea to use a neural network to approximate variables
that cannot be directly measured. We then applied this method to an
interesting physics problem and provided the first generic way of
measuring the VBS polarization fraction using a multi-variate
technique. The choice of a deep-learning algorithm in this case was
motivated by gains seen by references [14] and [15], in situations where
high-level variables are not known. Other regression techniques
such as a single layer neural network could be used. A discussion of
the merits of deep versus single neural networks is not intended to be
the focus of this paper. Other papers addressing this topic can be
found in the literature, for example, [14] and [15]

Following are some specific comments and suggestions that should to be
addressed.

1. Page 2, paragraph 2: The authors suggest that multivariate
techniques have not been used directly to measure underlying physics
quantities of interest. MVA regression techniques have been used by
Tevatron and LHC experiments (CMS) to estimate energy scale and energy
resolution of photons and jets. This is no different from using MVA
regression to directly measure a physics quantity. (Monte Carlo
studies have been made to measure top mass or Higgs mass from event
kinematics using MVA regression.)

===> Thank you for bringing this to our attention, since we were not
able to find this information publicly available.  We changed our text
to read:
“While it has become a common practice in high energy physics to use
multi-variate techniques to separate signal from background,
multi-variate regression is not commonly used to measure underlying
physics quantities of interest.  Unlike classification where the goal
of the neural network is to produce discrete assignments, for example
signal and background, NN regression relies on the fact that a neural
network can serve as a universal approximator, to instead approximate
an unknown continuous function. The goal of our NN is to find the best
approximation of the two truth values of cos theta* (one for each W
boson) present in each event, using measurable quantities.”

2. How many events were generated and used for deep network training?
Did the training of single layer network also use the same number of
events? What was the architecture of the best single layer network?
The 20% improvement in cost value, presumably is for events generated
at the parton-level?

===> 500,000 events were used for training and another million for
testing and validation.  We added this information to the text:
"500,000 events were used for training and another million for testing
and validation". Single layer training was done with the same
data. The architecture of the single layer network is the same as that
of the deep network with 14 input and 2 output variables.  Indeed the
20% improvement in cost value is for events at the parton-level.

3. Fig. 1: In the text, authors refer to Fig. 1(a), 1(b) etc. and the
plots in figures are not labeled as such. Also, the ordinate labeled
as ?A.U.? This should be changed to, say, ?Events (arbitrary units)?.

===> We added (a), (b) and (c) labels to Figure 1.  A.U. represents
arbitrary units; we added this definition in the caption.

4. Fig. 2.: The top and bottom axes are confusing. Suggest remaking
the plot with gaps between groups to indicate axes values of -1 and +1
appropriately, or have five different plots arranged side-by-side.
Cos(theta)_2^NN values should be shown as -1., -0.6, .. etc., not as a
continuous variable as the top axis presently suggests.

===> We followed your suggestion and have added additional space
between groups and modified the labels.

5. The deep network results with events generated at parton-level
yield a 68% confidence level limit on the LL fraction of (6.7 +/-
1.4)% whereas with modeling of detector simulation, this becomes (7^+5
-6)%. This result is then compared to results obtained using the
single R_pT variable (which does not have good discrimination). What
would the result be with single layer neural network using the same
variables as the deep network? How much gain does the deep neural
network provide relative to the shallow network in the scenario with
detector modeling? What is the expected effect of the pile-up problem
at the HL-LHC on the significance of the result and the relative gain?

===> The focus of this paper is to use the neural network regression
technique to approximate a physics quantity that cannot be directly
measured and to apply it on the physics problem that is directly
related to electroweak symmetry symmetry breaking. Since there can be
many different ways and parameters to train single and multiple layer
network, we choose to not directly compare the results.  In this case
the gains in the sensitivity for the deep network were similar to the
decrease in cost.
In regards to the question of pile-up two concerns have been brought
up. One is mis-identifying the tagging jets, the other is effects on
missing Et resolution. Mis-tagged jets in this case are not a major
concern, since they are generally easily rejected by increased pt
cuts. This can be performed with almost no loss of efficiency because
of the already relatively high M(j,j) cuts. We rely on delphes for the
MissingET resolution, which could miss some effects of high
pile-up. We feel that the mitigation of pile-up is an interesting
topic which requires accurate simulation and detector specific
reconstruction which we do not (and cannot) investigate in this paper.
We pointed out this limitation by adding the statement: "This detector
smearing adds some degree of realism, but could neglect various
effects due to the large number of overlapping interactions during the
high luminosity LHC runs. Since these effects are often mitigated with
specific reconstruction techniques and need detailed detector
modeling, we leave studies of this nature to dedicated efforts by the
experiments."



===========================================
=====List of changes to the manuscript=====
===========================================


Added text to Figure 1 caption, enclosed between **:
-"$R_{pT}$ templates for the corresponding polarization states *with
 log scaling* (b)"
-"...distributions...*are shown with arbitrary units (A.U.).*"
-"*(a) and (c) are the projections of the 2D distributions onto one of
 the two leptons, which are identical to the projections onto the
 other lepton due to arbitrary sorting.*"

Added (a), (b) and (c) labels to Figure 1.

Corrected the label of the x-axis to cos theta NN in Figure 1 (c).

Added three 2D plots to show the differences in cos theta NN 1 vs. 2
correlations as new Figure 2.

Updated the x-axis labels to "Integrated Luminosity" on Figure 4.

Added a sentence motivating the comparison of scenarios (a), (b) and (c):
"A comparison of scenarios (a) and (b) to (c) illustrates potential
gains in sensitivity through cut optimization and improved detector
performance."

Added sentence qualifying the delphes detector simulation used regarding
pileup modeling:
"This detector smearing adds some degree of realism, but could neglect
various effects due to the large number of overlapping interactions
during the high luminosity LHC runs. Since these effects are often
mitigated with specific reconstruction techniques and need detailed
detector modeling, we leave studies of this nature to dedicated
efforts by the experiments."


Modified the paragraph:
"While it has become a common practice in high energy physics to use
multi-variate techniques to separate signal from background, to the
authors' knowledge multi-variate regression has not been used to
directly measure underlying physics quantities of interest. Unlike
classification where the goal of the neural network is to produce
discrete assignments, for example signal and background, NN regression
relies on the fact that a neural network can serve as a universal
approximator~\cite{NN_1}, to instead approximate an unknown continuous
function.  The goal of our NN is to find the best
approximation using measurable quantities of the two truth values
of \cts (one for each $W$ boson) present in each event."
to read
"While it has become a common practice in high energy physics to use
multi-variate techniques to separate signal from background,
multi-variate regression is not commonly used to measure underlying
physics quantities of interest.  Unlike classification where the goal
of the neural network is to produce discrete assignments, for example
signal and background, NN regression relies on the fact that a neural
network can serve as a universal approximator, to instead approximate
an unknown continuous function. The goal of our NN is to find the best
approximation of the two truth values of cos theta* (one for each W
boson) present in each event, using measurable quantities."

Added more information on NN training:
"500,000 events were used for training and another million for testing
and validation"
